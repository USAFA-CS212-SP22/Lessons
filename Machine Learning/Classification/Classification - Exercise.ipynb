{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "* Describe when classification is the appropriate analysis technique\n",
    "* Use scikit-learn to perform classification\n",
    "\n",
    "\n",
    "## Digit Classification\n",
    "The [Digit Classification Problem](https://www.kaggle.com/c/digit-recognizer) is one of the most popular datasets for machine learning classification problems. We will start on this dataset with our knowledge of KNN and Logistic Regression classification techniques. Some of the leading predictions use Convolutional Neural Networks (CNN) and other Deep Learning Techniques.\n",
    "\n",
    "![digits](images/digits.png)\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Gather and Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Relevant Libraries\n",
    "**Q1.1** From sklearn import the digits dataset `load_digits`, import our two models (KNN and Logistic Regression), import the test-train split library, and the metrics library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "**Q1.2.** Save a \"Bunch\" object containing the digits dataset and attributes and assign the returned object to `digits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Data\n",
    "**Q1.3** Inspect the `digits` bunch object to see what the data, targets and attributes look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
       " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
       " 'frame': None,\n",
       " 'feature_names': ['pixel_0_0',\n",
       "  'pixel_0_1',\n",
       "  'pixel_0_2',\n",
       "  'pixel_0_3',\n",
       "  'pixel_0_4',\n",
       "  'pixel_0_5',\n",
       "  'pixel_0_6',\n",
       "  'pixel_0_7',\n",
       "  'pixel_1_0',\n",
       "  'pixel_1_1',\n",
       "  'pixel_1_2',\n",
       "  'pixel_1_3',\n",
       "  'pixel_1_4',\n",
       "  'pixel_1_5',\n",
       "  'pixel_1_6',\n",
       "  'pixel_1_7',\n",
       "  'pixel_2_0',\n",
       "  'pixel_2_1',\n",
       "  'pixel_2_2',\n",
       "  'pixel_2_3',\n",
       "  'pixel_2_4',\n",
       "  'pixel_2_5',\n",
       "  'pixel_2_6',\n",
       "  'pixel_2_7',\n",
       "  'pixel_3_0',\n",
       "  'pixel_3_1',\n",
       "  'pixel_3_2',\n",
       "  'pixel_3_3',\n",
       "  'pixel_3_4',\n",
       "  'pixel_3_5',\n",
       "  'pixel_3_6',\n",
       "  'pixel_3_7',\n",
       "  'pixel_4_0',\n",
       "  'pixel_4_1',\n",
       "  'pixel_4_2',\n",
       "  'pixel_4_3',\n",
       "  'pixel_4_4',\n",
       "  'pixel_4_5',\n",
       "  'pixel_4_6',\n",
       "  'pixel_4_7',\n",
       "  'pixel_5_0',\n",
       "  'pixel_5_1',\n",
       "  'pixel_5_2',\n",
       "  'pixel_5_3',\n",
       "  'pixel_5_4',\n",
       "  'pixel_5_5',\n",
       "  'pixel_5_6',\n",
       "  'pixel_5_7',\n",
       "  'pixel_6_0',\n",
       "  'pixel_6_1',\n",
       "  'pixel_6_2',\n",
       "  'pixel_6_3',\n",
       "  'pixel_6_4',\n",
       "  'pixel_6_5',\n",
       "  'pixel_6_6',\n",
       "  'pixel_6_7',\n",
       "  'pixel_7_0',\n",
       "  'pixel_7_1',\n",
       "  'pixel_7_2',\n",
       "  'pixel_7_3',\n",
       "  'pixel_7_4',\n",
       "  'pixel_7_5',\n",
       "  'pixel_7_6',\n",
       "  'pixel_7_7'],\n",
       " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
       " 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 5620\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect digits\n",
    "digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the `digits` object contains: `data`, `target`, `frame`, `feature_names`, `target_names`, `images`, and `DESCR` (description). Read more about the [digit dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits) to understand what each sample contains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.4.** Inspect the first five data samples and their targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0., 12., 10.,  0.,  0.,  0.,  0.,  0.,  0., 14., 16., 16.,\n",
       "       14.,  0.,  0.,  0.,  0., 13., 16., 15., 10.,  1.,  0.,  0.,  0.,\n",
       "       11., 16., 16.,  7.,  0.,  0.,  0.,  0.,  0.,  4.,  7., 16.,  7.,\n",
       "        0.,  0.,  0.,  0.,  0.,  4., 16.,  9.,  0.,  0.,  0.,  5.,  4.,\n",
       "       12., 16.,  4.,  0.,  0.,  0.,  9., 16., 16., 10.,  0.,  0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect features\n",
    "digits.data[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect targets\n",
    "digits.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.5.** Assign the features `data` to `X` and the `targets` to `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store feature matrix in X\n",
    "X = digits.data\n",
    "\n",
    "# store response vector in y\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.6.** How many digit samples do we have? What does each feature represent? What targets are we trying to predict? *Hint: look at the shape of the data, the feature_names, and the target_names*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of X\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of y\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.6. Answers**\n",
    "1. 1797\n",
    "2. a pixel\n",
    "3. digit 0-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data\n",
    "**Q1.7.** Split the digits data into training and testing sets. Save 20% of the data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Choose and Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.1.** Create and fit three models:\n",
    "1. KNN (K=1)\n",
    "2. KNN (K=5)\n",
    "3. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (K=1) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create knn object\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# fit model\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (K=5) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create knn object\n",
    "knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# fit model\n",
    "knn5.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "**NOTE: We need to change the maximum number of iterations for our Logistic Regression object. Add `max_iter=5000` as a paramter to your object instantiation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=5000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create logistic regression object\n",
    "logreg = LogisticRegression(max_iter=5000)\n",
    "\n",
    "# fit model\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluate Models\n",
    "**Q3.1.** Find model predictions on `X_test` and calculate classification accuracy for the three models:\n",
    "1. KNN (K=1)\n",
    "2. KNN (K=5)\n",
    "3. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (K=1) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9833333333333333"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints classification accuracy\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN (K=5) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9833333333333333"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints classification accuracy\n",
    "knn5.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9805555555555555"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints classification accuracy\n",
    "logreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Tune Models\n",
    "It looks like the KNN models performed the best, so lets tune the KNN model to find the right value for K.\n",
    "\n",
    "**Q4.1.** Find and even better value for K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9833333333333333,\n",
       " 0.9833333333333333,\n",
       " 0.9805555555555555,\n",
       " 0.9833333333333333,\n",
       " 0.9833333333333333,\n",
       " 0.9861111111111112,\n",
       " 0.9777777777777777,\n",
       " 0.9833333333333333,\n",
       " 0.9777777777777777,\n",
       " 0.9861111111111112,\n",
       " 0.9833333333333333,\n",
       " 0.9888888888888889,\n",
       " 0.9861111111111112,\n",
       " 0.9888888888888889,\n",
       " 0.9888888888888889,\n",
       " 0.9888888888888889,\n",
       " 0.9833333333333333,\n",
       " 0.9833333333333333,\n",
       " 0.9777777777777777,\n",
       " 0.9805555555555555,\n",
       " 0.9805555555555555,\n",
       " 0.9805555555555555,\n",
       " 0.9777777777777777,\n",
       " 0.9777777777777777,\n",
       " 0.9777777777777777]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "# try K=1 through K=25 and record classification accuracy\n",
    "for K in range(1,26):\n",
    "    # create knn object\n",
    "    knn = KNeighborsClassifier(n_neighbors=K)\n",
    "\n",
    "    # fit model\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # prints classification accuracy\n",
    "    scores.append(knn.score(X_test, y_test))\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.2.** Plot the classification accuracy for all values of K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the relationship between K and testing accuracy\n",
    "import numpy as np\n",
    "K = list(range(1, 26))\n",
    "sns.barplot(x=K, y=scores)\n",
    "plt.yticks(np.arange(min(scores), max(scores), 0.005))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.3.** Which values of K should we consider using?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. K = ___\n",
    "2. K = ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
